{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining New Autograd Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 8228.85644531 at run 0\n",
      "loss 863.779418945 at run 100\n",
      "loss 354.865844727 at run 200\n",
      "loss 277.007415771 at run 300\n",
      "loss 260.123199463 at run 400\n",
      "loss 254.526748657 at run 500\n",
      "loss 251.433700562 at run 600\n",
      "loss 249.037063599 at run 700\n",
      "loss 246.916259766 at run 800\n",
      "loss 244.953201294 at run 900\n",
      "CPU times: user 232 ms, sys: 8 ms, total: 240 ms\n",
      "Wall time: 233 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "\n",
    "class MyRelu(torch.autograd.Function):\n",
    "    \"\"\"\n",
    "    Implementing Custom grads\n",
    "    \"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def forward(ctx, input):\n",
    "        # ctx is  a cache object in which we can store \n",
    "        # current values of forward activation\n",
    "        ctx.save_for_backward(input)\n",
    "        return input.clamp(min=0)\n",
    "    \n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        input, = ctx.saved_tensors\n",
    "        grad_input = grad_output.clone()\n",
    "        grad_input[input < 0] = 0\n",
    "        return grad_input\n",
    "    \n",
    "\n",
    "H_size, In_size, Out_size = 4, 5, 2\n",
    "    \n",
    "W1 = Variable(torch.rand(H_size, In_size), requires_grad = True)\n",
    "W2 = Variable(torch.rand(Out_size, H_size), requires_grad = True)\n",
    "\n",
    "M = 1000 # no of training examples\n",
    "\n",
    "x_in = Variable(torch.rand(In_size, M), requires_grad = False) \n",
    "y = Variable(torch.rand(Out_size, M), requires_grad = False)\n",
    "\n",
    "learning_rate = 1e-6\n",
    "\n",
    "for t in range(1000):\n",
    "    relu = MyRelu.apply\n",
    "    # forward pass\n",
    "    out = W2.mm(relu(W1.mm(x_in)))\n",
    "    \n",
    "    # calculate loss\n",
    "    loss = (out - y).pow(2).sum()\n",
    "    \n",
    "    if t%100==0:\n",
    "        print(\"loss {0} at run {1}\".format(loss.data[0], t))\n",
    "    \n",
    "    # backprop\n",
    "    loss.backward()\n",
    "    \n",
    "    # param update\n",
    "    W1.data -= learning_rate * W1.grad.data\n",
    "    W2.data -= learning_rate * W2.grad.data\n",
    "    \n",
    "    # setting in graph grads zero\n",
    "    W1.grad.data.zero_()\n",
    "    W2.grad.data.zero_()\n",
    "    #repeat"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deeplearning",
   "language": "python",
   "name": "deeplearning"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
