{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Long Short Term Memory Nets in Pytorch\n",
    "\n",
    "We have seen various feed-forward networks. That is, there is no state maintained by the network at all. This might not be the behavior we want. Sequence models are central to NLP: they are models where there is some sort of dependence through time between your inputs. The classical example of a sequence model is the Hidden Markov Model for part-of-speech tagging. Another example is the conditional random field.\n",
    "\n",
    "A recurrent neural network is a network that maintains some kind of state. For example, its output could be used as part of the next input, so that information can propogate along as the network passes over the sequence. In the case of an LSTM, for each element in the sequence, there is a corresponding hidden state `ht`, which in principle can contain information from arbitrary points earlier in the sequence. We can use the hidden state to predict words in a language model, part-of-speech tags, and a myriad of other things.\n",
    "\n",
    "**_Before getting to the example, note a few things. Pytorchâ€™s LSTM expects all of its inputs to be 3D tensors. The semantics of the axes of these tensors is important. The first axis is the sequence itself, the second indexes instances in the mini-batch, and the third indexes elements of the input. _**\n",
    "\n",
    "An example:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f2317ff2590>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.autograd as autograd\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    " \n",
    "    \n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('OUT', Variable containing:\n",
      "(0 ,.,.) = \n",
      " -0.4696  0.0255  0.2446\n",
      "\n",
      "(1 ,.,.) = \n",
      " -0.4593  0.2174  0.1853\n",
      "\n",
      "(2 ,.,.) = \n",
      " -0.4381  0.2919  0.2162\n",
      "\n",
      "(3 ,.,.) = \n",
      " -0.5070  0.2419  0.1992\n",
      "\n",
      "(4 ,.,.) = \n",
      " -0.4843  0.1492  0.1986\n",
      "[torch.FloatTensor of size 5x1x3]\n",
      ")\n",
      "('HIDDEN', (Variable containing:\n",
      "(0 ,.,.) = \n",
      " -0.4843  0.1492  0.1986\n",
      "[torch.FloatTensor of size 1x1x3]\n",
      ", Variable containing:\n",
      "(0 ,.,.) = \n",
      " -0.8128  0.2265  0.2971\n",
      "[torch.FloatTensor of size 1x1x3]\n",
      "))\n"
     ]
    }
   ],
   "source": [
    "lstm = nn.LSTM(3,3)\n",
    "inputs = [ autograd.Variable(torch.rand(1, 3)) for _ in\n",
    "        range(5)] # 5 instances of 3-number sequence\n",
    "hidden = (autograd.Variable(torch.randn(1, 1, 3)),   # initialize_hidden\n",
    "          autograd.Variable(torch.randn((1, 1, 3))))\n",
    "\n",
    "for i in inputs:\n",
    "    out, hidden = lstm(i.view(1, 1, -1), hidden) # every-time we feed in new input vector and \n",
    "                                                 # and previous history state\n",
    "\n",
    "inputs = torch.cat(inputs).view(len(inputs), 1, -1)\n",
    "hidden = ( autograd.Variable(torch.randn(1, 1, 3)), \n",
    "           autograd.Variable(torch.randn(1, 1, 3)) )\n",
    "out, hidden = lstm(inputs, hidden)\n",
    "print(\"OUT\", out)\n",
    "print(\"HIDDEN\", hidden)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deeplearning",
   "language": "python",
   "name": "deeplearning"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
