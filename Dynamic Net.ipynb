{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch Dynamic Net\n",
    "\n",
    "We have random number of hidden layers between the input and output layer, this is difficult to produce in tf cause of static graph flow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 10809.3037109375)\n",
      "(101, 10809.6201171875)\n",
      "(201, 10808.646484375)\n",
      "(301, 10803.3681640625)\n",
      "(401, 10800.1630859375)\n",
      "(501, 10798.25)\n",
      "(601, 10804.7919921875)\n",
      "(701, 10789.37890625)\n",
      "(801, 10756.958984375)\n",
      "(901, 10777.904296875)\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "\n",
    "class DynamicNet(torch.nn.Module):\n",
    "    def __init__(self, In_size, H_size, Out_size):\n",
    "        super(DynamicNet, self).__init__()\n",
    "        self.input_layer = torch.nn.Linear(In_size, H_size)\n",
    "        self.middle_layer = torch.nn.Linear(H_size, H_size)\n",
    "        self.out_layer = torch.nn.Linear(H_size, Out_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        h = self.input_layer(x).clamp(min = 0)\n",
    "        for _ in range(random.randint(0,10)):\n",
    "            h = self.middle_layer(h).clamp(min = 0)\n",
    "        out = self.out_layer(h)\n",
    "        return out\n",
    "    \n",
    "M, In_size, H_size, Out_size = 10000, 5, 4, 2\n",
    "\n",
    "x = Variable(torch.rand(M, In_size), requires_grad = False)  # Row is taking different example\n",
    "y = Variable(torch.rand(M, Out_size), requires_grad = False) # that's how it's defined in package, \n",
    "                                                            # so operation will be col major\n",
    "\n",
    "model = DynamicNet(In_size, H_size, Out_size)\n",
    "\n",
    "loss = torch.nn.MSELoss(size_average = False)\n",
    "\n",
    "learning_rate = 1e-6\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = learning_rate)\n",
    "for t in range(1000):\n",
    "    out = model(x)\n",
    "    loss_out = loss(out, y)\n",
    "    if t%100 == 1:\n",
    "        print(t, loss_out.data[0])\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    loss_out.backward()\n",
    "    \n",
    "    optimizer.step()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deeplearning",
   "language": "python",
   "name": "deeplearning"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
